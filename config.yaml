# Codegate Example Configuration

# Network settings
port: 8989           # Port to listen on (1-65535)
host: "localhost"      # Host to bind to (use localhost for all interfaces)

# Logging configuration
log_level: "INFO"  # One of: ERROR, WARNING, INFO, DEBUG

# Note: This configuration can be overridden by:
# 1. CLI arguments (--port, --host, --log-level)
# 2. Environment variables (CODEGATE_APP_PORT, CODEGATE_APP_HOST, CODEGATE_APP_LOG_LEVEL)


# Embedding model configuration

####
# Inference model configuration
##

# Model to use for chatting
model_base_path: "./codegate_volume/models"

# Context length of the model
chat_model_n_ctx: 32768

# Number of layers to offload to GPU. If -1, all layers are offloaded.
chat_model_n_gpu_layers: -1

# Embedding model
embedding_model: "all-minilm-L6-v2-q5_k_m.gguf"